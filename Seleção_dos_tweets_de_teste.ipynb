{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# instala pacotes necessarios para o resto das opera√ß√µes\n",
        "!pip install datetime ijson"
      ],
      "metadata": {
        "id": "IdqTzHT2zq-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9457625-9d53-4551-ebab-31142f71b3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: ijson in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.1.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (75.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import decimal\n",
        "import pandas as pd\n",
        "import ijson\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "user_sample_ids_path = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test.csv'\n",
        "\n",
        "# Carrega arquivo de ids da amostra\n",
        "sample_ids = pd.read_csv(user_sample_ids_path)\n",
        "\n",
        "# Limpa ids da amostra\n",
        "sample_ids['id'] = sample_ids['id'].astype(int)"
      ],
      "metadata": {
        "id": "sIG8QblFEvSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gera amostra de tweets"
      ],
      "metadata": {
        "id": "w1URA3Zv2NGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho dos arquivos\n",
        "input_file_template = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_{}.json'\n",
        "output_file_template = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_{}.json'\n",
        "\n",
        "# Carregar os author_ids do CSV\n",
        "print(\"Carregando author_ids do CSV...\")\n",
        "author_ids = set(sample_ids['id'])\n",
        "print(f\"Total de author_ids carregados: {len(author_ids)}\")\n",
        "\n",
        "def filter_large_json_iteratively(input_file_path, output_file_path, author_ids):\n",
        "    print(f\"Iniciando filtragem do arquivo JSON: {input_file_path}...\")\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as input_file, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "\n",
        "        parser = ijson.items(input_file, 'item')\n",
        "        filtered_data = []\n",
        "        item_count = 0\n",
        "\n",
        "        for item in parser:\n",
        "            if item.get('author_id') in author_ids:\n",
        "                entities_obj = item.get('entities') or {}\n",
        "                user_mentions = entities_obj.get('user_mentions') or []\n",
        "                # selecione s√≥ os atributos importantes para an√°lise\n",
        "                new_item = {\n",
        "                    'id': item.get('id').lstrip('t'),\n",
        "                    'text': item.get('text'),\n",
        "                    'author_id': str(item.get('author_id')),\n",
        "                    'created_at': item.get('created_at'),\n",
        "                    'referenced_tweets': item.get('referenced_tweets') or [],\n",
        "                    'entities': {\n",
        "                        'mentions': user_mentions\n",
        "                    },\n",
        "                    'public_metrics': item.get('public_metrics') or {},\n",
        "                    'in_reply_to_user_id': item.get('in_reply_to_user_id')\n",
        "                }\n",
        "                filtered_data.append(new_item)\n",
        "            item_count += 1\n",
        "\n",
        "        print(f\"Total de itens processados: {item_count}\")\n",
        "        print(f\"Total de itens filtrados: {len(filtered_data)}\")\n",
        "\n",
        "        # Exibe exemplo de dado filtrado\n",
        "        if filtered_data:\n",
        "            print(f\"Exemplo de dado filtrado: {filtered_data[0]}\")\n",
        "        else:\n",
        "            print(\"Nenhum dado filtrado dispon√≠vel para mostrar.\")\n",
        "\n",
        "        # Salva os dados filtrados em um arquivo JSON\n",
        "        try:\n",
        "            print(\"Tentando serializar os dados filtrados para JSON...\")\n",
        "            json_string = json.dumps(filtered_data, ensure_ascii=False, indent=4)\n",
        "            output_file.write(json_string)\n",
        "            print(f\"Dados filtrados gravados em {output_file_path}\")\n",
        "        except (TypeError, ValueError) as e:\n",
        "            print(f\"Erro ao serializar dados para JSON: {e}\")\n",
        "\n",
        "inicio = datetime.datetime.now()\n",
        "print(f\"Hor√°rio de in√≠cio: {inicio}\")\n",
        "\n",
        "for i in range(0, 9):  # Processa os arquivos de tweet_0.json a tweet_8.json\n",
        "    print(f\"\\nProcessando arquivo {i}...\")\n",
        "    input_file_path = input_file_template.format(i)\n",
        "    output_file_path = output_file_template.format(i)\n",
        "    filter_large_json_iteratively(input_file_path, output_file_path, author_ids)\n",
        "    print(f\"Arquivo {i} processado e salvo em {output_file_path}\")\n",
        "\n",
        "fim = datetime.datetime.now()\n",
        "print(f\"Hor√°rio de fim: {fim}\")"
      ],
      "metadata": {
        "id": "QQn3pUfU2cT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256afa3e-3096-4ac0-bae4-9d7426d79556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando author_ids do CSV...\n",
            "Total de author_ids carregados: 1000\n",
            "Hor√°rio de in√≠cio: 2024-11-17 22:22:53.535783\n",
            "\n",
            "Processando arquivo 0...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_0.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 7357\n",
            "Exemplo de dado filtrado: {'id': '1479435410761822209', 'text': 'RT @Fra_Ciardo: NEW PAPER OUT! Check it on @JCgntn üëâRobot‚Äôs Social Gaze Affects Conflict Resolution but not Conflict Adaptations: https://t‚Ä¶', 'author_id': '557267440', 'created_at': '2022-01-07 12:51:03+00:00', 'referenced_tweets': [], 'entities': {'mentions': [{'screen_name': 'Fra_Ciardo', 'name': 'Francesca Ciardo ', 'id': 1405082118, 'id_str': '1405082118', 'indices': [3, 14]}, {'screen_name': 'JCgntn', 'name': 'Journal of Cognition', 'id': 854681597827842050, 'id_str': '854681597827842050', 'indices': [43, 50]}]}, 'public_metrics': {'retweet_count': 6, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_0.json\n",
            "Arquivo 0 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_0.json\n",
            "\n",
            "Processando arquivo 1...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_1.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 7111\n",
            "Exemplo de dado filtrado: {'id': '710313249208717312', 'text': \"@poss_theBOSS lol I'm honored! Might have to start using this thing just for you!\", 'author_id': '2382470575', 'created_at': '2016-03-17 03:54:10+00:00', 'referenced_tweets': [], 'entities': {'mentions': []}, 'public_metrics': {'retweet_count': 0, 'reply_count': None, 'like_count': 1, 'quote_count': None}, 'in_reply_to_user_id': 1377289459}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_1.json\n",
            "Arquivo 1 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_1.json\n",
            "\n",
            "Processando arquivo 2...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_2.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 7639\n",
            "Exemplo de dado filtrado: {'id': '1491825979857289217', 'text': 'RT @IAmAaronWill: Age: 18 to 25\\n\\nThe age where you can build your life or destroy it.', 'author_id': '1484104198191034370', 'created_at': '2022-02-10 17:26:45+00:00', 'referenced_tweets': [], 'entities': {'mentions': [{'screen_name': 'IAmAaronWill', 'name': 'Aaron', 'id': 2968471852, 'id_str': '2968471852', 'indices': [3, 16]}]}, 'public_metrics': {'retweet_count': 646, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_2.json\n",
            "Arquivo 2 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_2.json\n",
            "\n",
            "Processando arquivo 3...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_3.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 6327\n",
            "Exemplo de dado filtrado: {'id': '1265649158817538049', 'text': '@Muthu3DPrinting Hey Muthu! Sincere congratulations on the design and the safety key, where can I find the design files?', 'author_id': '119834526', 'created_at': '2020-05-27 14:20:48+00:00', 'referenced_tweets': [], 'entities': {'mentions': [{'screen_name': 'Muthu3DPrinting', 'name': 'Muthu Vellayappan', 'id': 960466287871672321, 'id_str': '960466287871672321', 'indices': [0, 16]}]}, 'public_metrics': {'retweet_count': 0, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'in_reply_to_user_id': 960466287871672321}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_3.json\n",
            "Arquivo 3 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_3.json\n",
            "\n",
            "Processando arquivo 4...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_4.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 7990\n",
            "Exemplo de dado filtrado: {'id': '1491717692457467904', 'text': 'NEJM: Re SARSCOV2 https://t.co/83zX8K40rH\\n\\n\"protection of prev infection against hospitalization/death caused by reinfection appeared to be robust regardless of variant\"\\n\\nOffit: This is yet another paper that proves that natural infection protects you against severe illness https://t.co/gGBzkymIpA', 'author_id': '139173680', 'created_at': '2022-02-10 10:16:27+00:00', 'referenced_tweets': [], 'entities': {'mentions': []}, 'public_metrics': {'retweet_count': 11, 'reply_count': None, 'like_count': 23, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_4.json\n",
            "Arquivo 4 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_4.json\n",
            "\n",
            "Processando arquivo 5...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_5.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 8671\n",
            "Exemplo de dado filtrado: {'id': '1498127505991028740', 'text': 'RT @aman_chokshi: Sometimes, living at the #SouthPole can feel like living on an alien planet. An inhospitable, beautiful white world. Noth‚Ä¶', 'author_id': '1955077080', 'created_at': '2022-02-28 02:46:45+00:00', 'referenced_tweets': [], 'entities': {'mentions': [{'screen_name': 'aman_chokshi', 'name': 'Aman Chokshi', 'id': 1178228662920867845, 'id_str': '1178228662920867845', 'indices': [3, 16]}]}, 'public_metrics': {'retweet_count': 10, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_5.json\n",
            "Arquivo 5 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_5.json\n",
            "\n",
            "Processando arquivo 6...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_6.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 8250\n",
            "Exemplo de dado filtrado: {'id': '1495940128182870028', 'text': 'O terceiro gol do SanS√£o foi essa BOMBA do Rodrigo Nestor e n√£o teve como, voc√™s elegeram como o Gola√ßo da 8¬™ rodada.\\n#Paulist√£o22 #FutebolPaulista https://t.co/DX6Mk5dzVL', 'author_id': '179937965', 'created_at': '2022-02-22 01:54:54+00:00', 'referenced_tweets': [], 'entities': {'mentions': []}, 'public_metrics': {'retweet_count': 2, 'reply_count': None, 'like_count': 15, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_6.json\n",
            "Arquivo 6 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_6.json\n",
            "\n",
            "Processando arquivo 7...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_7.json...\n",
            "Total de itens processados: 10000000\n",
            "Total de itens filtrados: 8224\n",
            "Exemplo de dado filtrado: {'id': '1500131173091139591', 'text': 'RT @Capitan_trueeno: De nuevo por aqu√≠ despu√©s de que el pajarraco me haya tumbado la cuenta por ESPA√ëOL https://t.co/Uah4d3vS3w', 'author_id': '1875276487', 'created_at': '2022-03-05 15:28:37+00:00', 'referenced_tweets': [], 'entities': {'mentions': [{'screen_name': 'Capitan_trueeno', 'name': 'Capitan Truenoüíöüíöüá™üá¶üá™üá¶', 'id': 1499833978374828043, 'id_str': '1499833978374828043', 'indices': [3, 19]}]}, 'public_metrics': {'retweet_count': 382, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_7.json\n",
            "Arquivo 7 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_7.json\n",
            "\n",
            "Processando arquivo 8...\n",
            "Iniciando filtragem do arquivo JSON: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_8.json...\n",
            "Total de itens processados: 8217457\n",
            "Total de itens filtrados: 6376\n",
            "Exemplo de dado filtrado: {'id': '1496908709884473349', 'text': 'CCB team of Greater Chennai police arrested Baskar and Kamesh who forged the signature of the owner of a private company in Chennai and committed fraud to the tune of 1.09 crore.\\n\\nhttps://t.co/9Q2a6OnjD2\\n\\n#chennaipolice \\n#greaterchennaipolice \\n#chennaicitypolice\\n#shankarjiwalips', 'author_id': '1406885239', 'created_at': '2022-02-24 18:03:42+00:00', 'referenced_tweets': [], 'entities': {'mentions': []}, 'public_metrics': {'retweet_count': 0, 'reply_count': None, 'like_count': 3, 'quote_count': None}, 'in_reply_to_user_id': None}\n",
            "Tentando serializar os dados filtrados para JSON...\n",
            "Dados filtrados gravados em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_8.json\n",
            "Arquivo 8 processado e salvo em /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_8.json\n",
            "Hor√°rio de fim: 2024-11-17 22:50:45.618477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforma amostra de tweets para o formato esperado pelo sistema"
      ],
      "metadata": {
        "id": "ueh2GECl3M-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def twitter_tweet_to_tweet(twitter_tweet: dict) -> dict:\n",
        "    id = twitter_tweet['id']\n",
        "    text = twitter_tweet['text']\n",
        "    author_id = twitter_tweet['author_id']\n",
        "    referenced_tweets = twitter_tweet.get('referenced_tweets') or []\n",
        "    entities = twitter_tweet.get('entities') or {}\n",
        "    public_metrics = twitter_tweet['public_metrics']\n",
        "    in_reply_to_user_id = twitter_tweet.get('in_reply_to_user_id')\n",
        "    created_at = twitter_tweet['created_at']\n",
        "\n",
        "    retweet_count = public_metrics.get('retweet_count', 0)\n",
        "    like_count = public_metrics.get('like_count', 0)\n",
        "    reply_count = public_metrics.get('reply_count', 0)\n",
        "    quote_count = public_metrics.get('quote_count', 0)\n",
        "\n",
        "    mentions = [\n",
        "        {'id': mention['id'], 'username': mention['screen_name']}\n",
        "        for mention in entities.get('mentions', [])\n",
        "    ]\n",
        "\n",
        "    is_reply = in_reply_to_user_id is not None and in_reply_to_user_id != author_id\n",
        "\n",
        "    if not referenced_tweets or len(referenced_tweets) == 0:\n",
        "        is_retweet = text.startswith(\"RT\")\n",
        "    else:\n",
        "        is_retweet = any(tweet['type'] == 'retweeted' for tweet in referenced_tweets if tweet)\n",
        "\n",
        "    return {\n",
        "        'id': id,\n",
        "        'isReply': is_reply,\n",
        "        'isRetweet': is_retweet,\n",
        "        'mentions': mentions,\n",
        "        'text': text,\n",
        "        'authorId': author_id,\n",
        "        'nRetweet': retweet_count,\n",
        "        'nLike': like_count,\n",
        "        'nReply': reply_count,\n",
        "        'nQuote': quote_count,\n",
        "        'tweetCreatedAt': created_at\n",
        "    }\n",
        "\n",
        "def process_tweets():\n",
        "    for i in range(9):\n",
        "        file_path = f'/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_{i}.json'\n",
        "        output_file_path = f'/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_{i}.json'\n",
        "        print(f\"Lendo arquivo: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            tweets = json.load(file)\n",
        "            processed_tweets = [twitter_tweet_to_tweet(tweet) for tweet in tweets]\n",
        "            print(f\"Escrevendo tweets processados em: {output_file_path}\")\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "              json.dump(processed_tweets, output_file, ensure_ascii=False, indent=4)\n",
        "              print(\"Tweets processados foram adicionados com sucesso.\")\n",
        "\n",
        "inicio = datetime.datetime.now()\n",
        "print(f\"Hor√°rio de in√≠cio: {inicio}\")\n",
        "process_tweets()\n",
        "fim = datetime.datetime.now()\n",
        "print(f\"Hor√°rio de fim: {fim}\")"
      ],
      "metadata": {
        "id": "fURcuN2YOIMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0bf0af-1407-4e24-bce6-4b2b2b5f83d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hor√°rio de in√≠cio: 2024-11-18 00:30:07.103762\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_0.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_0.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_1.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_1.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_2.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_2.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_3.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_3.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_4.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_4.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_5.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_5.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_6.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_6.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_7.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_7.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Lendo arquivo: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/test_tweet_8.json\n",
            "Escrevendo tweets processados em: /content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_test_tweet_8.json\n",
            "Tweets processados foram adicionados com sucesso.\n",
            "Hor√°rio de fim: 2024-11-18 00:30:10.394335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fgnua11nQJFl"
      }
    }
  ]
}