{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# instala pacotes necessarios para o resto das operações\n",
        "!pip install datetime ijson"
      ],
      "metadata": {
        "id": "IdqTzHT2zq-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import decimal\n",
        "import pandas as pd\n",
        "import ijson\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "user_sample_ids_path = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/user_sample_ids.csv'\n",
        "\n",
        "# Carrega arquivo de ids da amostra\n",
        "sample_ids = pd.read_csv(user_sample_ids_path)\n",
        "\n",
        "# Limpa ids da amostra\n",
        "sample_ids['id'] = sample_ids['id'].str.lstrip('u').astype(int)"
      ],
      "metadata": {
        "id": "sIG8QblFEvSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gera amostra de tweets"
      ],
      "metadata": {
        "id": "w1URA3Zv2NGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho dos arquivos\n",
        "input_file_template = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/tweet_{}.json'\n",
        "output_file_template = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/sample_tweet_{}.json'\n",
        "\n",
        "# Carregar os author_ids do CSV\n",
        "print(\"Carregando author_ids do CSV...\")\n",
        "author_ids = set(sample_ids['id'])\n",
        "print(f\"Total de author_ids carregados: {len(author_ids)}\")\n",
        "\n",
        "def filter_large_json_iteratively(input_file_path, output_file_path, author_ids):\n",
        "    print(f\"Iniciando filtragem do arquivo JSON: {input_file_path}...\")\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as input_file, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "\n",
        "        parser = ijson.items(input_file, 'item')\n",
        "        filtered_data = []\n",
        "        item_count = 0\n",
        "\n",
        "        for item in parser:\n",
        "            if item.get('author_id') in author_ids:\n",
        "                entities_obj = item.get('entities') or {}\n",
        "                user_mentions = entities_obj.get('user_mentions') or []\n",
        "                # selecione só os atributos importantes para análise\n",
        "                new_item = {\n",
        "                    'id': item.get('id').lstrip('t'),\n",
        "                    'text': item.get('text'),\n",
        "                    'author_id': str(item.get('author_id')),\n",
        "                    'created_at': item.get('created_at'),\n",
        "                    'referenced_tweets': item.get('referenced_tweets') or [],\n",
        "                    'entities': {\n",
        "                        'mentions': user_mentions\n",
        "                    },\n",
        "                    'public_metrics': item.get('public_metrics') or {},\n",
        "                    'in_reply_to_user_id': item.get('in_reply_to_user_id')\n",
        "                }\n",
        "                filtered_data.append(new_item)\n",
        "            item_count += 1\n",
        "\n",
        "        print(f\"Total de itens processados: {item_count}\")\n",
        "        print(f\"Total de itens filtrados: {len(filtered_data)}\")\n",
        "\n",
        "        # Exibe exemplo de dado filtrado\n",
        "        if filtered_data:\n",
        "            print(f\"Exemplo de dado filtrado: {filtered_data[0]}\")\n",
        "        else:\n",
        "            print(\"Nenhum dado filtrado disponível para mostrar.\")\n",
        "\n",
        "        # Salva os dados filtrados em um arquivo JSON\n",
        "        try:\n",
        "            print(\"Tentando serializar os dados filtrados para JSON...\")\n",
        "            json_string = json.dumps(filtered_data, ensure_ascii=False, indent=4)\n",
        "            output_file.write(json_string)\n",
        "            print(f\"Dados filtrados gravados em {output_file_path}\")\n",
        "        except (TypeError, ValueError) as e:\n",
        "            print(f\"Erro ao serializar dados para JSON: {e}\")\n",
        "\n",
        "inicio = datetime.datetime.now()\n",
        "print(f\"Horário de início: {inicio}\")\n",
        "\n",
        "for i in range(0, 9):  # Processa os arquivos de tweet_0.json a tweet_8.json\n",
        "    print(f\"\\nProcessando arquivo {i}...\")\n",
        "    input_file_path = input_file_template.format(i)\n",
        "    output_file_path = output_file_template.format(i)\n",
        "    filter_large_json_iteratively(input_file_path, output_file_path, author_ids)\n",
        "    print(f\"Arquivo {i} processado e salvo em {output_file_path}\")\n",
        "\n",
        "fim = datetime.datetime.now()\n",
        "print(f\"Horário de fim: {fim}\")"
      ],
      "metadata": {
        "id": "QQn3pUfU2cT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforma amostra de tweets para o formato esperado pelo sistema"
      ],
      "metadata": {
        "id": "ueh2GECl3M-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def twitter_tweet_to_tweet(twitter_tweet: dict) -> dict:\n",
        "    id = twitter_tweet['id']\n",
        "    text = twitter_tweet['text']\n",
        "    author_id = twitter_tweet['author_id']\n",
        "    referenced_tweets = twitter_tweet.get('referenced_tweets') or []\n",
        "    entities = twitter_tweet.get('entities') or {}\n",
        "    public_metrics = twitter_tweet['public_metrics']\n",
        "    in_reply_to_user_id = twitter_tweet.get('in_reply_to_user_id')\n",
        "    created_at = twitter_tweet['created_at']\n",
        "\n",
        "    retweet_count = public_metrics.get('retweet_count', 0)\n",
        "    like_count = public_metrics.get('like_count', 0)\n",
        "    reply_count = public_metrics.get('reply_count', 0)\n",
        "    quote_count = public_metrics.get('quote_count', 0)\n",
        "\n",
        "    mentions = [\n",
        "        {'id': mention['id'], 'username': mention['screen_name']}\n",
        "        for mention in entities.get('mentions', [])\n",
        "    ]\n",
        "\n",
        "    is_reply = in_reply_to_user_id is not None and in_reply_to_user_id != author_id\n",
        "\n",
        "    if not referenced_tweets or len(referenced_tweets) == 0:\n",
        "        is_retweet = text.startswith(\"RT\")\n",
        "    else:\n",
        "        is_retweet = any(tweet['type'] == 'retweeted' for tweet in referenced_tweets if tweet)\n",
        "\n",
        "    return {\n",
        "        'id': id,\n",
        "        'isReply': is_reply,\n",
        "        'isRetweet': is_retweet,\n",
        "        'mentions': mentions,\n",
        "        'text': text,\n",
        "        'authorId': author_id,\n",
        "        'nRetweet': retweet_count,\n",
        "        'nLike': like_count,\n",
        "        'nReply': reply_count,\n",
        "        'nQuote': quote_count,\n",
        "        'tweetCreatedAt': created_at\n",
        "    }\n",
        "\n",
        "def process_tweets():\n",
        "    for i in range(9):\n",
        "        file_path = f'/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/sample_tweet_{i}.json'\n",
        "        output_file_path = f'/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_tweets_{i}.json'\n",
        "        print(f\"Lendo arquivo: {file_path}\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            tweets = json.load(file)\n",
        "            processed_tweets = [twitter_tweet_to_tweet(tweet) for tweet in tweets]\n",
        "            print(f\"Escrevendo tweets processados em: {output_file_path}\")\n",
        "            with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
        "              json.dump(processed_tweets, output_file, ensure_ascii=False, indent=4)\n",
        "              print(\"Tweets processados foram adicionados com sucesso.\")\n",
        "\n",
        "inicio = datetime.datetime.now()\n",
        "print(f\"Horário de início: {inicio}\")\n",
        "process_tweets()\n",
        "fim = datetime.datetime.now()\n",
        "print(f\"Horário de fim: {fim}\")"
      ],
      "metadata": {
        "id": "fURcuN2YOIMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fgnua11nQJFl"
      }
    }
  ]
}