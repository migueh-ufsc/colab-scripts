{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sII070-GD43v",
        "outputId": "d118629e-762d-4a67-a381-60744b18a597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processando arquivo 0...\n",
            "Arquivo 0 processado com sucesso!\n",
            "\n",
            "Processando arquivo 1...\n",
            "Arquivo 1 processado com sucesso!\n",
            "\n",
            "Processando arquivo 2...\n",
            "Arquivo 2 processado com sucesso!\n",
            "\n",
            "Processando arquivo 3...\n",
            "Arquivo 3 processado com sucesso!\n",
            "\n",
            "Processando arquivo 4...\n",
            "Arquivo 4 processado com sucesso!\n",
            "\n",
            "Processando arquivo 5...\n",
            "Arquivo 5 processado com sucesso!\n",
            "\n",
            "Processando arquivo 6...\n",
            "Arquivo 6 processado com sucesso!\n",
            "\n",
            "Processando arquivo 7...\n",
            "Arquivo 7 processado com sucesso!\n",
            "\n",
            "Processando arquivo 8...\n",
            "Arquivo 8 processado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "file_count = 0\n",
        "\n",
        "def split_large_json(input_file, output_dir, max_size_mb=100):\n",
        "  \"\"\"Splits a large JSON file into smaller JSON files.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input JSON file.\n",
        "    output_dir: Path to the directory where the split files will be saved.\n",
        "    max_size_mb: Maximum size of each output file in MB.\n",
        "  \"\"\"\n",
        "\n",
        "  global file_count # Declare file_count as global within the function\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  with open(input_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  current_file_data = []\n",
        "  current_file_size = 0\n",
        "\n",
        "  for item in data:\n",
        "    item_size = len(json.dumps(item))\n",
        "    if current_file_size + item_size > max_size_mb * 1024 * 1024:\n",
        "      output_file = os.path.join(output_dir, f'split_{file_count}.json')\n",
        "      with open(output_file, 'w') as outfile:\n",
        "        json.dump(current_file_data, outfile, indent=2)\n",
        "      file_count += 1\n",
        "      current_file_data = []\n",
        "      current_file_size = 0\n",
        "\n",
        "    current_file_data.append(item)\n",
        "    current_file_size += item_size\n",
        "\n",
        "  if current_file_data:\n",
        "    output_file = os.path.join(output_dir, f'split_{file_count}.json')\n",
        "    with open(output_file, 'w') as outfile:\n",
        "      json.dump(current_file_data, outfile, indent=2)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/split_files'\n",
        "input_file_template = '/content/drive/MyDrive/Colab Notebooks/Twibot22 - Dataset completo/parsed_tweets_{}.json'\n",
        "\n",
        "for i in range(0, 9):\n",
        "    print(f\"\\nProcessando arquivo {i}...\")\n",
        "    input_file_path = input_file_template.format(i)\n",
        "    split_large_json(input_file_path, output_dir)\n",
        "    print(f\"Arquivo {i} processado com sucesso!\")"
      ]
    }
  ]
}